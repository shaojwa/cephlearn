#### 部署【块存储】集群
#### 在【pool信息】下的【块存储】新建pool【blkpool0】
- 【节点池】：nodepool0
- 【硬盘池】：diskpool0
- 【冗余策略】：副本
- 【副本个数】：3
- 【单副本可读】：是
- 【最小可写冗余策略】：数据可靠性优先。

【最小可写冗余策略】
- 如果是【数据可靠性优先】，至少有存在一份冗余数据，才允许写入，也就是副本数至少为2。
- 如果是【业务连续性优先】，就没这个要求，哪怕只有一个副本，也允许写入，所以如果那个副本故障，那就存在数据丢失的风险。

#### 选择【块存储】下【卷管理】下的【存储卷】，创建卷：lun0
【新建】，选择【存储池】：blkpool0，【卷名称】：lun0，【存储卷大小】102400G

注意，如果这个存储池是厚配置，存储卷大小必须小于等于存储池可用容量。

#### 选择【块存储】下的【卷映射】下的【业务主机】，配置【配置块服务网段】
选择【IPv4块服务网段】：172.16.21.0/24，选择【IPv6块服务网段】：不填

#### 选择【块存储】下的【卷映射】下的【业务主机】，选择【新建】。
【名称】blkhost0，【操作系统】：linux，【启动器】：通过在集群外的一台业务主机上运行`cat /etc/iscsi/initiatorname.iscsi`查出

#### 选择【块存储】下的【卷映射】下的【业务主机组】，选择【新建】。
【节点池】：nodepool0，【名称】：hostg0，

#### 选择【块存储】下的【卷映射】下的【业务主机组】下，点击新创建的业务主机组hostg0
将集群外的节点node70，node71添加进去。

#### 选择【块存储】下的【卷映射】下的【映射管理】下，点击新创建的业务主机组hostg0，在【卷】标签页下选择【添加】
将lun0，lun1添加到业务主机组。

#### 此时在业务主机172.12.21.70去发现iscsi的目标（target），无法发现。
```
$ sudo iscsiadm -m discovery -t st -p 172.16.21.73
iscsiadm: cannot make connection to 172.16.21.73: Connection refused

-m，表示mode，op是discovery。
-t指定type，st是SendTargets，针对原生的iSCSI协议。
-p指定提供iscsi服务的主机。
```

#### 如果在网络发现集群里的iscsi目标，那么在【端口管理-卷映射】下【扫描网卡】发现一下端口。
扫描出网卡之后，一般就可以在同一个网段内的主机上发现。
```
[root@node70 SDS_Admin]# sudo iscsiadm -m discovery -t st -p 172.16.21.73
172.16.21.73:3260,1 iqn.2018-01.com.h3c.onestor:1f370e6107474f82a56d7810199c1f50
[root@node70 SDS_Admin]# sudo iscsiadm -m discovery -t st -p 172.16.21.74
172.16.21.74:3260,1 iqn.2018-01.com.h3c.onestor:1f370e6107474f82a56d7810199c1f50
[root@node70 SDS_Admin]# sudo iscsiadm -m discovery -t st -p 172.16.21.75
172.16.21.75:3260,1 iqn.2018-01.com.h3c.onestor:1f370e6107474f82a56d7810199c1f50
```

#### 用发现的TargetName去login
```
[root@node70 ~]# iscsiadm -m node -T iqn.2018-01.com.h3c.onestor:1f370e6107474f82a56d7810199c1f50 -l
```

#### login之后就可以在业务机上通过lsblk查看到：
```
[root@node70 SDS_Admin]# lsblk
NAME        MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT
sdc           8:32   0   100G  0 disk
sdd           8:48   0   100G  0 disk
sde           8:64   0   100G  0 disk
sdf           8:80   0   100G  0 disk
sdg           8:96   0   100G  0 disk
sdh           8:112  0   100G  0 disk
```

#### 如果查不到磁盘，考虑下是否将当前主机加入到主机组
即使主机没有加入到主机组，那么这个主机还是可以discover到Target，但是通过主机组的portal进行login的时候，不会成功，lsblk就会查询不到。
所以，如果login失败，就需要检查下。

#### 发现挂载的磁盘多于集群中的卷数量
那是因为，在集群每个节点上，都有两个卷可以挂载，而当前的业务主机组没有添加端口，需要在【映射管理】中添加【端口】。
在默认情况，如果一个业务主机组里没有添加端口，似乎会把所有可用的端口都挂载一遍，所以挂了三次。

当然，还有一个办法是，在login的时候（-l参数），所以需要指定portal（这里就是 -p 172.16.21.73）来login
```
sudo iscsiadm -m node -T iqn.2018-01.com.h3c.onestor:1f370e6107474f82a56d7810199c1f50 -l -p 172.16.21.73
```

#### 查看发现的Target
```
sudo iscsiadm -m node
```

#### 在业务机上logout
```
sudo iscsiadm -m node -T iqn.2018-01.com.h3c.onestor:20c5611213f64c0d9500f7096d9f390e -u
```

#### 如果想logout所有的Target
```
$ iscsiadm -m node --logoutall=all
$ iscsiadm -m node -U all // 等价于
```

#### 在业务机上刷新iscsi会话
```
sudo iscsiadm -m session -R
```
#### 如果要查看每个磁盘关联的集群主机
```
$ ll /dev/diskby-path
```
