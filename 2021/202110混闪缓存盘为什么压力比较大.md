#### 现象
4M纯写，11个HDD，加2个SSD，但是发现SSD的负载很高（%util）。

#### osd 数量
```
编号从0-44，一共45个。
dcache池 个节点2个osd，一共6个。
row池 每个节点2个osd，一共6个。
data 个节点用11个osd，一共33个。
```

#### 分析
```
sdo, sdn两个SSD盘，每个ssd有23个分区。
其中两个分区（sdn4， sdn2）有对应的osd进程。
一个分区是是dcache在用（osd9），node1.dcache下，是100M+100G。
一个是ROW在用（osd7），在node1.row下。我们可以看到这个osd所在的盘
```

#### 查看osd中的ROW和db的盘符
```
$ cd /var/lib/ceph/osd/ceph-1
```
直接看block这个软连接，可以看到
```
ll /dev/disk/by-partuuid/137c7db9-807e-437f-a6fb-007fe40b2d52
```
就可以看到这个uuid对应的分区。或者还有一个办法，但是比较麻烦，先找到uuid之后，然后在/dev/disk/by-partuuid/里找，其实是同一回事。
找到对应的元uuid
```
$ cat block_uuid
```
然后用这个uuid在下面的目录下找：
```
$ cd /dev/disk/by-partuuid/
```

#### 如果有元数据分离部署
那么每个data-osd下的`/var/lib/ceph/osd/ceph-N`下会多两个软连接：
```
block.db  
block.wal 
```
wal一般是2G，db是36G，SCACHE是72G。

#### ROW池是干什么用的

