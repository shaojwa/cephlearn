# 摘要

* 将读写操作从元数据操作中解耦对PB规模的分布式文件系统来说至关重要。
* 动态子树分区以及自适应元数据管理系统来高效的管理分层的元数据负载。

# 1 介绍

* 尽管元数据大小占整个文件系统大小的比例很低但是元数据的操作数量却占到50%左右。
* 相比OSD容量可以通过增加OSD来进行扩展，MDS因为相互之间更高的依赖性而使得设计更加具有挑战性。
* MDS集群需要在各种负载场景下，都能高效的管理文件系统目录以及权限，不管是科学计算还是通用计算。

### 文件系统负载

* 需要支持文件大小范围从几字节到TB字节，目录包含百万级文件，数千个客户端同时访问不同或者相同的文件。
* 工作负载也可能随时改变，MDS集群，要能通过动态得拆分负载，来持续适应当前的需求，以保持高性能以及长期可扩展性。
* 我们已用一种动态子树分区策略来持续调整元数据分布。
* 通过模拟基本的系统性能，对文件系统以及负载的调整，来评估元数据管理策略。

# 2 背景

* 考察的场景包括几十个MDS，几千个OSD，以及成百上千个client。
* 负载场景包括科学计算，网络归档，大数据中心等几个典型的分布式文件系统应用场景。

### 2.1 系统架构

* 一个主要的设计特点是将元数据管理从数据管理中分离出来。
* 把元数据操作（这种依更加复杂且相互依赖的语义操作）从数据操作中分离出来，可以避免常规文件服务器的IO瓶颈。
*　元数据服务器的主要工作：文件系统命名空间，目录层次，文件以及目录权限管理，文件到对象的映射。

### 2.1.1 数据分布

* 确定的伪随机算法
* 一个重要的文件数据分布算法是，OSD的序列可以被客户端重新计算出来，而不需要和MDS交互。

### 2.1.2  元数据分区

* 元数据负载必须能高效得在元数据服务器之间分布，以应付极端的负载：山该案个客户端打开同一个文件，或者写入同一个目录。
*　不同MDS之间的元数据缓存重叠要尽可能的小。
* MDS集群需要提供故障恢复机制。
* MDS集群扩展的时候能尽可能少的进行负载重新分布。

### 2.1.3 存储

* 需要一个共享的TB级别的持久化存储。

## 2.2 负载

* 元数据事务分两类：
  * open,close,setattr等针对文件或者目录的inode。
  * rename，unlink，等在影响系统命名空间以及层次的directory dentry。
 
 # 3 相关工作
 
 ## 3.1 元数据分布方法
 
 ### 3.1.1 子树分区
 
 * 对path或者inode等唯一标识进行Hash。
 
 ### 3.1.2 hash
 
 这是现今很多文件系统采用的一种方式，以hash函数来计算元数据以及数据的位置。只要hash函数设计得好，这种方式有很多优点。
 
#### 优点：
 
 * client可以直接和元数据负责的MDS交互。
 * 请求会在集群内的多个MDS服务器之间均衡分布。
 * 在单一目录下的大量创建操作，不会出现访问热点（一个目录内文件的位置已经不再有局部性）。
  
 #### 缺点：
 
 * 由独立文件构成的热点还是有可能压垮一个负责的MDS服务器。
 * 随机的服务器元数据访问导致每个MDS服务器都缓存元数据。
 
 google通过只读的元数据副本来缓和因为多个客户端访问同一文件导致的热点，代价是需要极大得简化元数据语义。
 
 * hash会导致为适应增长而进行的MDS扩展变得非常困难
 
 * 更需要注意的是，hash消除了层次结构的局部性，市区了本地文件系统拥有的局部性好处。
 
 #### 另外：
 
 为了满足POSXI语义，必须需要对文件的上层目录进行元数据访问来确定用户是否有权权限访问当前目录，所以 hash方法会导致较高的开销。
 不管是访问分布在不同服务器上的元数据，还是缓存在本地的元数据副本。本地的缓存会有高度的重合，因为提供子节点的元数据就必须要缓存父节点的元数据。
 
 
 ### 3.1.3 Lazy Hybrid
 
 混合模式：
 
 * 用文件的全路径hash来分布元数据。
 * 为换个hash模式中的路径遍历导致的高开销，LH使用双入口访问控制列表来存储整个路径上每个文件的元数据信息。
 
 我们已经发现，就算是大型文件系统，浙西额信息都通常可以表示得非常简洁。
 LH只需要在访问控制列表需要更新时才遍历路径，比如其中个上层目录的访问权限改变时，影响到这层目录下嵌套的所有文件访问权限。
 类似得，rename或者move目录也会影响hash的输出，因此下面嵌套的所有文件的元数据位置以及元数据需要在MDS之间迁移。
 
 
# 4 动态元数据管理

我们提出了一种元数据集群架构可以充分利用一种动态子树分分区策略来在集群的多个MDS之间分布元数据。
基于子树的分区，是一种很自然的对分层结构进行分区的方式，相比hash方式，它可以提供很多优点，包括更好的MDS独立性，以及更好的访问局部性。
尽管，创建以及维护一个好的分区将更为困难。动态分区策略必须能适应文件系统以及负载的改变。
更进一步，相比其他技术，动态分区有一些列的优势，包括元数据存储，流量控制，以及更灵活的资源使用规则。

## 4.1 分层分区

文件系统通过对子树委托权威来实现，委托可能是嵌套的，/usr委托给一个MDS，而/usr/local可能委托给另外一个MDS。
尽管对目录层次较深的文件的定位来说，路径的遍历看起来是一个很昂贵的操作。
但是不管是通用计算还是科学计算，引用的局部性使得这个代价可以被后续的对相同目录的访问分摊掉。

更重要的是，不想LH中的权限管理，层次定义的结构允许系统移动或者改变任意大小的子树，只需要修改子树的根节点，这个代价几乎是固定的。
同样的，独立的子树是完全独立于它的兄弟节点，语义也仅仅依赖于引向文件系统根的祖先目录。

为了允许客户端的请求能高效得处理，每一个MDS会缓存所有节点的前缀节点（祖先节点），也就是说，只有可能是叶子节点可能过期。
目录只有在它下面的条目都过期之后，才有可能被移除。

## 4.2 权威以及辅助缓存

元数据的更新在某些节点上必须串行化，因为需要维持原子性和一致性。
经验告诉我们，约束简单的方法越有效，为了最大化平均负载，每一个元数据项都会有一个定义良好的权威节点。
这个节点负载串行化更新，提交更新到持久化缓存，以及管理缓存的一致性和完整性。
如果一个MDS收到一个请求，这个请求针对的层次并非是这个MDS负责时，这个MDS会转发这个请求给权威MDS。

如果他需要备份一些元数据（不关事因为这个目录受欢迎并被标记为备份，还是这个MDS需要一些前缀元数据去遍历它所负责的子树）
这个MDS会从相关的元数据权威节点上请求数据。一旦一个条目在另外一个MDS上作为副本缓存，权威MDS就有责任去告知更新一保持一致性。

类似的，如果一个节点放弃缓存一个inode，这个MDS也会告知这个inode的权威节点，这样这个inode的权威节点就可以自由的将自己的副本从内存中移除。
这种方式保证在MDS集群内部，文件系统的的状态会保持一致并且定义良好。

在某些情况下，更新元数据可能是分布式的，比如修改时间以及文件大小等字段，对大部分操作来说，通常是单调递增的。
这样，提供并发写的副本会周期性得发送最近的值到权威MDS上，权威MDS保留最大的值，并在客户端的读操作上，初始化一个最近信息的回调。
比如，stat 去校验文件大小，这种方式在PGFS文件系统中用来实现针对文件的共享并发写操作。

客户端之间的一致性一般会没有那么严格，一个系统服务数千个客户端，如果要为客户端提供强一致性，通常都需要非常大的内存来存储必要的状态信息。
相反得，弱一致性以及无状态方式，比如NFS（v3以及更早）会导致一系列客户端应用的问题。
我们相信，相对简单的元数据一致性策略可能是足够的，尽管合适的方式可能是依赖于特定的负载需求，这不再这篇文章的讨论范围内。

## 4.3 负载均衡











