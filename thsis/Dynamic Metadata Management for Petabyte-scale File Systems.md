# 摘要

* 将读写操作从元数据操作中解耦对PB规模的分布式文件系统来说至关重要。
* 动态子树分区以及自适应元数据管理系统来高效的管理分层的元数据负载。

# 1 介绍

* 尽管元数据大小占整个文件系统大小的比例很低但是元数据的操作数量却占到50%左右。
* 相比OSD容量可以通过增加OSD来进行扩展，MDS因为相互之间更高的依赖性而使得设计更加具有挑战性。
* MDS集群需要在各种负载场景下，都能高效的管理文件系统目录以及权限，不管是科学计算还是通用计算。

### 文件系统负载

* 需要支持文件大小范围从几字节到TB字节，目录包含百万级文件，数千个客户端同时访问不同或者相同的文件。
* 工作负载也可能随时改变，MDS集群，要能通过动态得拆分负载，来持续适应当前的需求，以保持高性能以及长期可扩展性。
* 我们已用一种动态子树分区策略来持续调整元数据分布。
* 通过模拟基本的系统性能，对文件系统以及负载的调整，来评估元数据管理策略。

# 2 背景

* 考察的场景包括几十个MDS，几千个OSD，以及成百上千个client。
* 负载场景包括科学计算，网络归档，大数据中心等几个典型的分布式文件系统应用场景。

### 2.1 系统架构

* 一个主要的设计特点是将元数据管理从数据管理中分离出来。
* 把元数据操作（这种依更加复杂且相互依赖的语义操作）从数据操作中分离出来，可以避免常规文件服务器的IO瓶颈。
*　元数据服务器的主要工作：文件系统命名空间，目录层次，文件以及目录权限管理，文件到对象的映射。

### 2.1.1 数据分布

* 确定的伪随机算法
* 一个重要的文件数据分布算法是，OSD的序列可以被客户端重新计算出来，而不需要和MDS交互。

### 2.1.2  元数据分区

* 元数据负载必须能高效得在元数据服务器之间分布，以应付极端的负载：山该案个客户端打开同一个文件，或者写入同一个目录。
*　不同MDS之间的元数据缓存重叠要尽可能的小。
* MDS集群需要提供故障恢复机制。
* MDS集群扩展的时候能尽可能少的进行负载重新分布。

### 2.1.3 存储

* 需要一个共享的TB级别的持久化存储。

## 2.2 负载

* 元数据事务分两类：
  * open,close,setattr等针对文件或者目录的inode。
  * rename，unlink，等在影响系统命名空间以及层次的directory dentry。
 
 # 3 相关工作
 
 ## 3.1 元数据分布方法
 
 ### 3.1.1 子树分区
 
 * 对path或者inode等唯一标识进行Hash。
 
 ### 3.1.2 hash
 
 这是现今很多文件系统采用的一种方式，以hash函数来计算元数据以及数据的位置。只要hash函数设计得好，这种方式有很多优点。
 
#### 优点：
 
 * client可以直接和元数据负责的MDS交互。
 * 请求会在集群内的多个MDS服务器之间均衡分布。
 * 在单一目录下的大量创建操作，不会出现访问热点（一个目录内文件的位置已经不再有局部性）。
  
 #### 缺点：
 
 * 由独立文件构成的热点还是有可能压垮一个负责的MDS服务器。
 * 随机的服务器元数据访问导致每个MDS服务器都缓存元数据。
 
 google通过只读的元数据副本来缓和因为多个客户端访问同一文件导致的热点，代价是需要极大得简化元数据语义。
 
 * hash会导致为适应增长而进行的MDS扩展变得非常困难
 
 * 更需要注意的是，hash消除了层次结构的局部性，市区了本地文件系统拥有的局部性好处。
 
 #### 另外：
 
 为了满足POSXI语义，必须需要对文件的上层目录进行元数据访问来确定用户是否有权权限访问当前目录，所以 hash方法会导致较高的开销。
 不管是访问分布在不同服务器上的元数据，还是缓存在本地的元数据副本。本地的缓存会有高度的重合，因为提供子节点的元数据就必须要缓存父节点的元数据。
 
 
 ### 3.1.3 Lazy Hybrid
 
 混合。
 
 
  

